{
 "cells": [
  {
   "cell_type": "raw",
   "id": "884ad011",
   "metadata": {},
   "source": [
    "In this assessment, you'll work on regression. Find a dataset and build a KNN regression and an OLS regression. Compare the two. How similar are they? Do they miss in different ways?\n",
    "\n",
    "Create a Jupyter Notebook with your models. At the end, in a Markdown cell, write a few paragraphs to describe the models' behaviors and why you favor one model or the other. Try to determine whether there is a situation where you would change your mind, or whether one is unambiguously better than the other. Lastly, try to note what it is about the data that causes the better model to outperform the weaker model. Submit a link to your Notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5874d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/chowd/OneDrive/Desktop/Datasets/Credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330faac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e18d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 60491.098\n",
      "Mean Absolute Error: 182.37\n",
      "Root Mean Squared Error: 245.9493809709632\n"
     ]
    }
   ],
   "source": [
    "# Creating the KNN MODEL\n",
    "# Preprocessing\n",
    "# Remove non-numeric columns if any\n",
    "numeric_data = data.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "# Handle missing values if any\n",
    "numeric_data.fillna(0,inplace= True) # Filling missing values with 0, you might choose another strategy\n",
    "\n",
    "# Split the dataset into features(X) and target variable(y)\n",
    "X = numeric_data.drop(columns=['Balance']) # Features\n",
    "y = numeric_data['Balance'] # Target variable\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size = 0.2, random_state= 42)\n",
    "\n",
    "# Model training:\n",
    "k=5 # choosing the number of neighbors\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_regressor.fit(X_train,y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared= False)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e8f16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an OLS regression model using the given dataset\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d69e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/chowd/OneDrive/Desktop/Datasets/Credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a56edb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Balance   R-squared:                       0.878\n",
      "Model:                            OLS   Adj. R-squared:                  0.876\n",
      "Method:                 Least Squares   F-statistic:                     403.9\n",
      "Date:                Sat, 02 Mar 2024   Prob (F-statistic):          6.82e-175\n",
      "Time:                        11:49:46   Log-Likelihood:                -2598.2\n",
      "No. Observations:                 400   AIC:                             5212.\n",
      "Df Residuals:                     392   BIC:                             5244.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -478.2006     56.743     -8.427      0.000    -589.760    -366.642\n",
      "Unnamed: 0     0.0013      0.070      0.018      0.986      -0.137       0.139\n",
      "Income        -7.5582      0.383    -19.738      0.000      -8.311      -6.805\n",
      "Limit          0.1258      0.053      2.370      0.018       0.021       0.230\n",
      "Rating         2.0633      0.795      2.594      0.010       0.500       3.627\n",
      "Cards         11.5949      7.078      1.638      0.102      -2.321      25.511\n",
      "Age           -0.8929      0.479     -1.862      0.063      -1.835       0.050\n",
      "Education      1.9984      2.603      0.768      0.443      -3.119       7.116\n",
      "==============================================================================\n",
      "Omnibus:                       96.526   Durbin-Watson:                   1.950\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              170.319\n",
      "Skew:                           1.397   Prob(JB):                     1.04e-37\n",
      "Kurtosis:                       4.553   Cond. No.                     3.71e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.71e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Remove non-numeric columns if any\n",
    "numeric_data = data.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "# Handle missing values if any\n",
    "numeric_data.fillna(0,inplace= True) # Filling missing values with 0, you might choose another strategy\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target variable(y)\n",
    "X = numeric_data.drop(columns=['Balance']) # Features\n",
    "X = sm.add_constant(X) ## Add a constant (intercept) to the model\n",
    "y = numeric_data['Balance']  # Target variable\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a793d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1171693.270570617\n",
      "Mean Absolute Error: 1002.1311360413029\n",
      "Root Mean Squared Error: 1082.447814248159\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for OLS\n",
    "X_test = sm.add_constant(X_test)\n",
    "y_pred = results.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared= False)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed9b4d4",
   "metadata": {},
   "source": [
    "OLS regression and k-NN regression are two different approaches to modeling relationships between predictors and a target variable. Here's a comparison of both:\n",
    "\n",
    "    Modeling Approach:\n",
    "        OLS Regression: OLS regression aims to find the linear relationship between the predictor variables and the target variable by minimizing the sum of squared differences between the observed and predicted values.\n",
    "        k-NN Regression: k-NN regression predicts the target variable by averaging the values of its k nearest neighbors in the predictor variable space.\n",
    "\n",
    "    Assumptions:\n",
    "        OLS Regression: OLS regression assumes a linear relationship between predictors and the target variable, normality of residuals, homoscedasticity, and independence of errors.\n",
    "        k-NN Regression: k-NN regression does not make strong assumptions about the underlying data distribution but relies heavily on the local structure of the data.\n",
    "\n",
    "    Interpretability:\n",
    "        OLS Regression: OLS regression provides coefficients for each predictor variable, which can be interpreted in terms of the change in the target variable for a one-unit change in the predictor, holding other predictors constant.\n",
    "        k-NN Regression: k-NN regression does not provide easily interpretable coefficients. It relies more on the \"black box\" approach of using the nearest neighbors to predict the target variable.\n",
    "\n",
    "    Performance and Generalization:\n",
    "        OLS Regression: OLS regression may perform well when the underlying relationship between predictors and the target variable is linear and when the assumptions are met.\n",
    "        k-NN Regression: k-NN regression may perform well when the relationship is non-linear or when the data does not adhere to the assumptions of OLS regression. However, it can be sensitive to the choice of k and may not generalize well to unseen data if the feature space is high-dimensional.\n",
    "\n",
    "    Missing in Different Ways:\n",
    "        OLS Regression: OLS regression may miss capturing non-linear relationships between predictors and the target variable. It may also perform poorly if the assumptions of linearity and normality are violated.\n",
    "        k-NN Regression: k-NN regression may miss capturing global patterns in the data if the nearest neighbors do not adequately represent the underlying distribution. It may also be sensitive to outliers and noise in the data.\n",
    "\n",
    "In summary, OLS regression and k-NN regression offer different approaches to modeling relationships in data. They may perform differently depending on the nature of the data and the assumptions underlying each method. While OLS regression provides interpretable coefficients and assumes linearity, k-NN regression offers a non-parametric approach that can capture complex relationships but may lack interpretability and robustness in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f4bfc20",
   "metadata": {},
   "source": [
    "In the ols model above the ols conditions are violated (Presence of Multicollinearity, Non-normality: Jarque Bera statistic is 170.319 with a probability of 1.04e-37: Since this p-value is less than .05, we reject the null hypothesis. Thus, we have sufficient evidence to say that this data has skewness and kurtosis that is significantly different from a normal distribution.) hence the Knn-regression is a better model for the given data. \n",
    "If the assumptions of OLS regression regarding normality of residuals is violated, k-NN regression may provide more robust predictions without relying on these assumptions.\n",
    "\n",
    "From the model summary results we find mse,mae,rmse is lower for knn-regression. Hence we can say knn-model is the better model wrt to the given dataset.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3c23b56",
   "metadata": {},
   "source": [
    "Determining whether OLS regression or k-NN regression is preferred depends on various factors including the nature of the data, the underlying relationship between predictors and the target variable, and the specific objectives of the analysis. Here are some situations where you might prefer OLS regression over k-NN regression:\n",
    "\n",
    "    Linear Relationships: If the relationship between predictors and the target variable is believed to be approximately linear, OLS regression may be preferred. OLS regression explicitly models this linear relationship and provides interpretable coefficients that quantify the effect of each predictor variable on the target variable.\n",
    "\n",
    "    Interpretability: OLS regression provides easily interpretable coefficients that represent the change in the target variable for a one-unit change in each predictor variable, holding other predictors constant. If interpretability of the model is important, OLS regression might be preferred over k-NN regression, which lacks easily interpretable coefficients.\n",
    "\n",
    "    Assumption of Linearity: If the relationship between predictors and the target variable is believed to be linear and the assumptions of OLS regression (e.g., normality of residuals, homoscedasticity) are reasonably met, OLS regression may provide reliable estimates of the coefficients and perform well.\n",
    "\n",
    "    Low-Dimensional Data: OLS regression can handle a large number of predictors (features) compared to the number of observations (samples) without suffering from the curse of dimensionality, which can be a limitation for k-NN regression.\n",
    "\n",
    "However, there are situations where k-NN regression might be preferred or more suitable:\n",
    "\n",
    "    Nonlinear Relationships: If the relationship between predictors and the target variable is highly nonlinear, k-NN regression may capture this nonlinearity more effectively compared to OLS regression. k-NN regression makes no assumption about the form of the relationship and can capture complex patterns in the data.\n",
    "\n",
    "    Non-Normality or Heteroscedasticity: If the assumptions of OLS regression regarding normality of residuals and homoscedasticity are violated, k-NN regression may provide more robust predictions without relying on these assumptions.\n",
    "\n",
    "    Local Patterns: If the underlying data exhibits local patterns or clusters where nearby observations are more similar in terms of the target variable, k-NN regression may be preferred as it captures these local structures effectively.\n",
    "\n",
    "In practice, the choice between OLS regression and k-NN regression depends on a careful consideration of the data characteristics, model assumptions, interpretability requirements, and the trade-offs between bias and variance in the modeling process. There is no one-size-fits-all answer, and the choice often involves empirical evaluation and validation of both approaches on the specific dataset and problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
